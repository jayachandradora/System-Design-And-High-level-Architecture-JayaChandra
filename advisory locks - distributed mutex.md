# advisory locks - distributed mutex

Letâ€™s slow down and build the understanding **from first principles**, step by step, with **one concrete purge example** and **real code**.

Iâ€™ll explain:

1. **What problem exists in distributed systems**
2. **What advisory lock really is internally**
3. **How multiple servers coordinate without talking to each other**
4. **Step-by-step execution with 3 pods**
5. **Concrete Java + SQL code**
6. **Why this completely solves your purge problem**

---

# 1ï¸âƒ£ The core problem in distributed jobs

You have:

* 3 pods (servers)
* Same Java code
* Same `@Scheduled` time
* Same database
* Same table: `emp_details`

At 2:00 AM:

```
Pod A â†’ starts purge
Pod B â†’ starts purge
Pod C â†’ starts purge
```

They **do not know about each other**.

Result:

* Multiple DELETEs
* Serializable conflicts
* Errors

---

# 2ï¸âƒ£ What an advisory lock REALLY is (internals)

Think of the database as holding a **global in-memory map**:

```
LOCK_KEY â†’ OWNER_SESSION
```

Example:

```
"emp_details_purge" â†’ Session 42 (Pod A)
```

This map:

* Lives inside the DB cluster
* Is shared across **all connections, all pods**
* Is managed by the database engine itself

âš ï¸ Important:

* Advisory locks are **not on tables**
* They are **just entries in this map**

---

# 3ï¸âƒ£ How coordination happens without servers talking

### All pods run this SQL:

```sql
SELECT pg_try_advisory_lock('emp_details_purge');
```

### Database logic (simplified):

```
if lock_key not present:
    store (lock_key â†’ this_session)
    return true
else:
    return false
```

Thatâ€™s it.
No network calls between pods.
No Kubernetes awareness.
Only the **database** is the coordinator.

---

# 4ï¸âƒ£ Step-by-step timeline (3 pods)

### â° Time = 02:00:00 AM

All pods wake up.

---

### ğŸŸ¢ Pod A

```
DB: "Is emp_details_purge free?"
DB: Yes
DB: Assign lock to Pod A session
DB â†’ Pod A: true
```

Pod A continues.

---

### ğŸ”´ Pod B

```
DB: "Is emp_details_purge free?"
DB: No (Pod A owns it)
DB â†’ Pod B: false
```

Pod B exits job.

---

### ğŸ”´ Pod C

Same as Pod B.

---

### ğŸŸ¢ Pod A runs purge

```sql
DELETE FROM emp_details
WHERE last_updated < now() - interval '30 days';
```

* DB row locks ONLY old rows
* New inserts/updates continue normally

---

### ğŸŸ¢ Pod A finishes

```sql
SELECT pg_advisory_unlock('emp_details_purge');
```

DB removes lock entry.

---

### ğŸ•‘ Next day

Same process again.

---

# 5ï¸âƒ£ Why this works in distributed systems

| Problem                    | Solution                            |
| -------------------------- | ----------------------------------- |
| No shared memory           | Database acts as shared coordinator |
| Pods donâ€™t know each other | They all know the DB                |
| Network partitions         | DB guarantees correctness           |
| Pod crash                  | Lock released when connection dies  |

---

# 6ï¸âƒ£ What happens if Pod A crashes?

Very important question.

* Advisory lock is **owned by DB session**
* Pod crashes â†’ DB connection closes
* DB automatically removes the lock
* Next pod can acquire it

âœ” No deadlocks
âœ” No manual cleanup

---

# 7ï¸âƒ£ Full working Java example (detailed)

---

## Step 1: Lock service (low level)

```java
@Component
public class AdvisoryLockService {

    private final JdbcTemplate jdbcTemplate;

    public AdvisoryLockService(JdbcTemplate jdbcTemplate) {
        this.jdbcTemplate = jdbcTemplate;
    }

    public boolean tryLock(String lockName) {
        return Boolean.TRUE.equals(
            jdbcTemplate.queryForObject(
                "SELECT pg_try_advisory_lock(hashtext(?))",
                Boolean.class,
                lockName
            )
        );
    }

    public void unlock(String lockName) {
        jdbcTemplate.execute(
            "SELECT pg_advisory_unlock(hashtext('" + lockName + "'))"
        );
    }
}
```

Why `hashtext()`?

* Converts string â†’ int lock key
* Same string = same lock across all pods

---

## Step 2: Purge job (business logic)

```java
@Scheduled(cron = "0 0 2 * * *")
public void purgeEmpDetails() {

    log.info("Starting emp_details purge job");

    if (!lockService.tryLock("emp_details_purge")) {
        log.info("Another pod is running purge. Exiting.");
        return;
    }

    try {
        log.info("Lock acquired. Purging old records.");

        int deleted = jdbcTemplate.update("""
            DELETE FROM emp_details
            WHERE last_updated < now() - interval '30 days'
        """);

        log.info("Deleted {} old records from emp_details", deleted);

    } finally {
        lockService.unlock("emp_details_purge");
        log.info("Lock released");
    }
}
```

---

# 8ï¸âƒ£ What happens to live traffic?

While purge runs:

| Operation         | Result  |
| ----------------- | ------- |
| INSERT new emp    | Works   |
| UPDATE recent emp | Works   |
| SELECT            | Works   |
| Another purge     | Skipped |

Because:

* Advisory lock affects **only code that asks for it**
* DELETE locks only the rows it deletes

---

# 9ï¸âƒ£ Why this avoids your original error

Original error:

```
could not serialize access due to concurrent update
```

Cause:

* Multiple DELETEs running concurrently

With advisory lock:

* Only one DELETE ever runs
* No serialization conflict
* No retries
* No failures

---

# ğŸ”‘ Key mental model (this is the â€œahaâ€)

> **Advisory lock = â€œBefore doing this job, ask the database for permission.â€**

And:

> **The database is the only place all servers can reliably agree.**

---

# Final summary (memorize this)

* Advisory lock is NOT a data lock
* It is a **distributed mutex**
* DB stores it
* Pods coordinate through DB
* Perfect for jobs
* Zero impact on live traffic

---

The **distributed mutex (advisory lock) is handled entirely by the PostgreSQL / YugabyteDB server**, **not** by your job servers.

## Where the mutex actually lives

### âŒ NOT on:

* Job server
* Spring Boot application
* Kubernetes
* JVM memory

### âœ… It lives in:

* **PostgreSQL / YugabyteDB**
* Inside the **database engine**
* Shared across **all connections from all servers**

---

## Why the database is the right place

In a distributed system:

| Component         | Shared by all pods? |
| ----------------- | ------------------- |
| JVM memory        | âŒ No                |
| Local file system | âŒ No                |
| Kubernetes pod    | âŒ No                |
| Database          | âœ… Yes               |

So the DB becomes the **single source of truth** for coordination.

---

## What PostgreSQL/Yugabyte actually does internally (simplified)

Inside the DB server, there is an internal structure like:

```
AdvisoryLockTable
-------------------------
Lock Key        | Session ID
-------------------------
emp_details_purge | 83421
```

* Session ID = DB connection
* If session dies â†’ entry is removed automatically

---

## How your job servers interact with it

Each job server:

1. Opens a DB connection
2. Executes:

   ```sql
   SELECT pg_try_advisory_lock(...)
   ```
3. DB checks internal lock table
4. DB returns `true` or `false`

The job server does **no coordination logic** itself.

---

## Example with your purge job

### Infrastructure

```
Pod A â”€â”
Pod B â”€â”¼â”€â”€â–º PostgreSQL / YugabyteDB â—„â”€â”€ distributed mutex
Pod C â”€â”˜
```

### Execution

```
Pod A â†’ lock acquired
Pod B â†’ lock denied
Pod C â†’ lock denied
```

Only Pod A proceeds.

---

## What happens if a pod crashes?

Very important and often misunderstood:

* Pod crashes
* DB connection closes
* PostgreSQL/Yugabyte automatically releases the advisory lock

âœ” No orphan locks
âœ” No manual cleanup
âœ” No deadlocks

---

## Why this is called a â€œdistributed mutexâ€

Because:

* Multiple independent processes
* On different machines
* Coordinate access
* Using a shared authority (DB)

That is exactly the definition of a distributed mutex.

---

## Final mental model (one-liner)

> **Advisory lock = â€œAsk the database for exclusive permission before running the job.â€**

The database enforces it.
The job servers simply obey it.

---
Youâ€™re welcome ğŸ™‚
You asked exactly the *right* questions, so here are **real, production-grade use cases** where **advisory locks (distributed mutex)** are *intentionally* used.

Iâ€™ll group them from **most common** to **advanced**, with concrete examples so you can immediately relate.

---

# 1ï¸âƒ£ Scheduled jobs / batch jobs (MOST COMMON)

### Problem

Multiple pods start the same job at the same time.

### Example

* Purge old data
* Rebuild aggregates
* Generate reports
* Archive records

### Solution

Use advisory lock to allow **only one executor**.

```text
Lock: "emp_details_purge"
```

âœ” Exactly your case
âœ” Used in almost every enterprise system

---

# 2ï¸âƒ£ Idempotent background processing

### Problem

Same event/message can be processed by multiple workers.

### Example

* Kafka consumer rebalance
* Retry of failed job
* Duplicate message delivery

### Use case

Ensure:

> â€œThis business operation runs only once.â€

### Example

```text
Lock: "process_order_12345"
```

Only one worker processes order `12345`.

---

# 3ï¸âƒ£ Data migration / backfill jobs

### Problem

Long-running migration accidentally started twice.

### Example

* Backfilling historical data
* Schema evolution scripts
* One-time data fixes

### Why advisory lock

âœ” Prevents accidental re-execution
âœ” Survives restarts
âœ” No extra infra

---

# 4ï¸âƒ£ Leader-only logic inside a microservice

### Problem

You want **leader behavior** without Kubernetes leader election.

### Example

* One service publishes metrics
* One service syncs config
* One service polls external API

### Example lock

```text
"metrics_publisher_leader"
```

The holder acts as leader.

---

# 5ï¸âƒ£ Preventing concurrent business operations

### Problem

Two requests try to modify the same logical entity.

### Example

* Payroll run
* Monthly invoice generation
* End-of-day settlement

### Example

```text
Lock: "payroll_run_2025_01"
```

âœ” Ensures only one payroll run happens.

---

# 6ï¸âƒ£ Coordinating multiple microservices

### Problem

Different services share the same DB and must not overlap work.

### Example

* Service A cleans data
* Service B aggregates data

Both coordinate via:

```text
Lock: "daily_data_maintenance"
```

---

# 7ï¸âƒ£ Cache rebuild / warm-up

### Problem

All pods try to rebuild cache after restart.

### Example

* Load reference data
* Warm in-memory cache

### With advisory lock

Only one pod rebuilds; others wait or skip.

---

# 8ï¸âƒ£ Rate-limiting critical operations (advanced)

### Problem

Limit concurrent execution of expensive DB jobs.

### Example

* Max 1 heavy report at a time
* Prevent DB overload

Advisory lock enforces global limit.

---

# 9ï¸âƒ£ Alternative to Quartz (lightweight)

### Problem

Quartz is heavy; you just need **single execution**.

### Advisory lock gives:

âœ” Simplicity
âœ” Reliability
âœ” No extra tables

Used by many teams instead of Quartz.

---

# 10ï¸âƒ£ Safety switch for manual operations

### Problem

Two engineers run the same script accidentally.

### Example

```sql
SELECT pg_try_advisory_lock('manual_cleanup');
```

Only one script proceeds.

---

# When advisory locks are a BAD idea âŒ

Important to know limits.

| Scenario                      | Why                        |
| ----------------------------- | -------------------------- |
| Cross-database coordination   | Locks are DB-local         |
| Non-cooperating tools         | They ignore advisory locks |
| Very fine-grained row locking | Use DB locks               |
| External systems (no DB)      | Use Redis / Zookeeper      |

---

# Advisory lock vs other distributed locks

| Lock Type                  | Complexity | Reliability |
| -------------------------- | ---------- | ----------- |
| Advisory lock              | Low        | High        |
| Redis lock                 | Medium     | Medium      |
| Zookeeper                  | High       | Very High   |
| Kubernetes leader election | Medium     | High        |

---

# Simple rule of thumb (memorize this)

> **If all participants already talk to the same database â†’ advisory lock is the best choice.**

---

# Final takeaway

Advisory locks are:

* Simple
* Safe
* Database-enforced
* Production-proven
* Ideal for jobs & coordination

They are not magic â€” just **well-placed shared memory inside the DB**.

---

If you want next:

* Compare advisory lock vs Redis lock in failure scenarios
* Learn how big companies avoid Quartz
* See how Yugabyte differs from Postgres
* Explore batch purge best practices

Hereâ€™s a **clear ASCII architecture diagram** showing **how advisory locks work as a distributed mutex** for your **purge job** in a multi-pod production environment.

---

## ğŸ—ï¸ Architecture: Distributed Purge Job with Advisory Lock

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Kubernetes Cluster    â”‚
                    â”‚                           â”‚
                    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
                    â”‚   â”‚  Pod A   â”‚            â”‚
                    â”‚   â”‚ (Job)    â”‚            â”‚
                    â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
                    â”‚        â”‚                  â”‚
                    â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”‚
                    â”‚   â”‚  Pod B   â”‚            â”‚
                    â”‚   â”‚ (Job)    â”‚            â”‚
                    â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
                    â”‚        â”‚                  â”‚
                    â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”‚
                    â”‚   â”‚  Pod C   â”‚            â”‚
                    â”‚   â”‚ (Job)    â”‚            â”‚
                    â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
                    â”‚        â”‚                  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚  JDBC / SQL
                             â”‚  pg_try_advisory_lock()
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     PostgreSQL / YugabyteDB       â”‚
                â”‚                                  â”‚
                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚   â”‚  Advisory Lock Table      â”‚  â”‚
                â”‚   â”‚  (Internal, in-memory)   â”‚  â”‚
                â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
                â”‚   â”‚  emp_details_purge  â†’ A  â”‚â—„â”€â”¼â”€â”€ Pod A owns lock
                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â”‚                                  â”‚
                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚   â”‚      emp_details table    â”‚  â”‚
                â”‚   â”‚                          â”‚  â”‚
                â”‚   â”‚  old rows (< 30 days) â—„â”€â”€â”¼â”€â”€â”¼â”€â”€ DELETE
                â”‚   â”‚  recent rows             â”‚  â”‚
                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Execution Flow (Step-by-Step)

```
TIME = 02:00 AM (cron fires)

Pod A â†’ SELECT pg_try_advisory_lock("emp_details_purge") â†’ TRUE
Pod B â†’ SELECT pg_try_advisory_lock("emp_details_purge") â†’ FALSE
Pod C â†’ SELECT pg_try_advisory_lock("emp_details_purge") â†’ FALSE
```

Only **Pod A proceeds**.

---

## ğŸ§¹ Purge Phase

```
Pod A
  |
  |-- DELETE FROM emp_details
  |   WHERE last_updated < now() - 30 days
  |
  |-- (DB locks only those old rows)
  |
  |-- INSERT / UPDATE by other services â†’ still allowed
```

---

## ğŸ”“ Completion

```
Pod A â†’ SELECT pg_advisory_unlock("emp_details_purge")

DB â†’ removes lock entry
Next scheduled run â†’ repeat
```

---

## ğŸ§  Key Things This Diagram Shows

âœ” Pods **do not talk to each other**
âœ” Database is the **single coordinator**
âœ” Advisory lock is **logical (not table/row lock)**
âœ” Business traffic is **not blocked**
âœ” Exactly **one job executor**

---

## ğŸ“Œ One-line mental model

```
Multiple servers â†’ One database â†’ One lock â†’ One job
```

Perfect â€” letâ€™s go **step by step**. Iâ€™ll explain **how leader election works**, why itâ€™s needed, and give **concrete examples with Kubernetes and Zookeeper** for your purge job scenario.

Weâ€™ll assume the same use case: **purge old records from `emp_details` table**.

---

# 1ï¸âƒ£ Why leader election?

In a distributed system with multiple pods:

* You **cannot rely on @Scheduled alone**, because every pod would execute the job.
* **You want only one pod to act as the â€œleaderâ€** and run the job.
* If the leader crashes, another pod must take over automatically.

Leader election solves this.

---

# 2ï¸âƒ£ Leader election patterns

There are three common approaches:

| Approach                         | Coordinator       | Example use                      |
| -------------------------------- | ----------------- | -------------------------------- |
| **Kubernetes Lease API**         | K8s API server    | Microservices inside K8s cluster |
| **Zookeeper / etcd**             | Zookeeper cluster | Complex HA jobs, cross-cluster   |
| **Spring Cloud Leader Election** | K8s / DB          | Spring Boot native               |

Weâ€™ll focus on **Kubernetes and Spring Boot** for simplicity.

---

# 3ï¸âƒ£ Using Kubernetes Lease API (built-in)

### Concept

* K8s provides a **`Lease` resource** in a namespace.
* Pods try to acquire the lease.
* Lease holder becomes **leader**.
* If leader pod dies, the lease expires, and another pod becomes leader.

### Step-by-step

1. Create a `Lease` in your namespace:

```yaml
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: emp-details-purge-leader
  namespace: default
spec:
  holderIdentity: ""
  leaseDurationSeconds: 30
```

2. Use **client code** in each pod to try to acquire the lease.

---

### Java Example with Spring Boot + K8s

```java
@Component
public class PurgeJobLeader {

    private final KubernetesClient k8sClient;

    public PurgeJobLeader(KubernetesClient k8sClient) {
        this.k8sClient = k8sClient;
    }

    @Scheduled(cron = "0 0 2 * * *")
    public void runPurgeJob() {

        // Try to acquire lease
        boolean isLeader = tryAcquireLease();
        if (!isLeader) {
            log.info("Another pod is leader. Skipping purge.");
            return;
        }

        log.info("Leader acquired. Running purge job...");
        purgeOldEmpDetails();
    }

    private boolean tryAcquireLease() {
        // Simplified: acquire Lease via K8s API
        // If lease holderIdentity is empty or expired, set it to this pod
        // Return true if success, false otherwise
        // Libraries like fabric8 Kubernetes client handle this
        return k8sClient.tryAcquireLease("emp-details-purge-leader");
    }

    private void purgeOldEmpDetails() {
        jdbcTemplate.update(
            "DELETE FROM emp_details WHERE last_updated < now() - interval '30 days'"
        );
    }
}
```

**Notes:**

* Only the pod holding the lease runs the purge.
* Lease automatically expires if pod dies â†’ failover.
* Lease duration can be tuned (30â€“60 seconds usually).

---

# 4ï¸âƒ£ Using Zookeeper for Leader Election

### Concept

* Zookeeper maintains a **special node (znode)** for leader election.
* All pods try to create the node (`/leader/jobName`).
* First to succeed becomes leader.
* Others watch the node and try again if it disappears.

### Flow

1. Pod A creates `/leader/emp_details_purge` â†’ becomes leader.
2. Pod B watches `/leader/emp_details_purge` â†’ waits.
3. Pod A finishes â†’ deletes znode â†’ Pod B tries â†’ becomes new leader if running.

---

### Java Example using Curator

```java
LeaderSelector leaderSelector = new LeaderSelector(client, "/leader/emp_details_purge", 
    new LeaderSelectorListenerAdapter() {
        @Override
        public void takeLeadership(CuratorFramework client) throws Exception {
            // This pod is now leader
            purgeOldEmpDetails();

            // Hold leadership until done
        }
    });

leaderSelector.autoRequeue(); // ensures continuous election
leaderSelector.start();
```

**Notes:**

* Very robust for distributed systems outside Kubernetes.
* Zookeeper ensures **exactly one leader**.
* Handles failover automatically.

---

# 5ï¸âƒ£ Comparison: Advisory Lock vs Leader Election

| Feature        | Advisory Lock             | Leader Election                     |
| -------------- | ------------------------- | ----------------------------------- |
| Coordinator    | DB                        | K8s / Zookeeper                     |
| Failover       | Automatic (lock released) | Automatic (new leader elected)      |
| Multiple jobs  | Separate locks            | Single leader can run multiple jobs |
| Implementation | Simple SQL                | Requires K8s client or ZK client    |
| Use case       | Simple batch jobs         | Long-running services, HA tasks     |

---

# 6ï¸âƒ£ Production recommendation

* **If jobs are short, isolated, DB-heavy** â†’ advisory lock is simpler.
* **If jobs are long-running, multiple jobs, or HA needed** â†’ leader election is better.
* You can **combine both**: leader pod acquires advisory lock to ensure no double execution even during failover.

---

Below is a **clear comparison** of **ShedLock**, **Advisory Lock**, **Optimistic Lock**, and **Pessimistic Lock**.
Iâ€™ve given **bullet summaries** first, followed by a **comparison table** for quick reference.

---

## ğŸ”¹ Bullet-Point Explanation

### **1. ShedLock**

* Used **only for scheduled tasks** in distributed systems
* Ensures **only one instance** of a scheduled job runs at a time
* Works across multiple nodes (microservices / pods)
* Uses **external storage** (DB, Redis, Mongo, etc.) to manage lock
* Common in **Spring Boot @Scheduled jobs**
* Not meant for row-level or transactional locking

**Use case:** Preventing cron jobs from running simultaneously on multiple servers

---

### **2. Advisory Lock**

* A **database-level lock** controlled explicitly by the application
* Not automatically tied to rows or transactions
* Can lock **any arbitrary resource** (e.g., user ID, job name)
* Other sessions must **cooperate** and check the lock
* Common in PostgreSQL, MySQL

**Use case:** Coordinating work across multiple processes using the same database

---

### **3. Optimistic Lock**

* Assumes **conflicts are rare**
* No lock during read
* Uses **versioning or timestamps**
* Fails on update if data was changed by another transaction
* Lightweight and scalable

**Use case:** High-read, low-write systems (e.g., REST APIs, web apps)

---

### **4. Pessimistic Lock**

* Assumes **conflicts are likely**
* Locks data immediately on read
* Prevents others from reading or writing
* Can reduce concurrency
* Implemented via `SELECT ... FOR UPDATE`

**Use case:** Financial transactions, inventory management

---

## ğŸ“Š Comparison Table

| Feature             | ShedLock                         | Advisory Lock                  | Optimistic Lock        | Pessimistic Lock          |
| ------------------- | -------------------------------- | ------------------------------ | ---------------------- | ------------------------- |
| Purpose             | Prevent duplicate scheduled jobs | Application-controlled locking | Detect write conflicts | Prevent concurrent access |
| Scope               | Distributed systems              | Database-wide                  | Row-level              | Row-level                 |
| Lock Type           | External lock (DB/Redis)         | Logical/manual lock            | Version check          | Physical DB lock          |
| Tied to Transaction | âŒ No                             | âš ï¸ Optional                    | âœ… Yes                  | âœ… Yes                     |
| Blocks Other Reads  | âŒ No                             | âŒ No                           | âŒ No                   | âœ… Sometimes               |
| Blocks Other Writes | âŒ No                             | âŒ No                           | âŒ No (fails on commit) | âœ… Yes                     |
| Performance Impact  | Low                              | Low                            | Very Low               | High                      |
| Failure Handling    | Lock timeout                     | Must release manually          | Retry on failure       | Wait or deadlock          |
| Common Technologies | Spring ShedLock                  | PostgreSQL, MySQL              | JPA/Hibernate          | SQL, JPA                  |
| Best For            | Scheduled jobs                   | Cross-process coordination     | High concurrency apps  | Critical data consistency |

---

## ğŸ§  Quick Recommendation

* **Scheduled jobs?** â†’ **ShedLock**
* **Cross-service coordination?** â†’ **Advisory Lock**
* **High concurrency web apps?** â†’ **Optimistic Lock**
* **Strict consistency (money, stock)?** â†’ **Pessimistic Lock**



